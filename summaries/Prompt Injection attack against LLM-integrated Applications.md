Liu et al. (2023b) addresses security vulnerabilities in Large Language Models like GPT-4, focusing on prompt injection attacks. It introduces the HOUYI methodology, a blackbox prompt injection attack approach designed for versatility and adaptability across various LLMintegrated services/applications. HOUYI comprises three phases: Context Inference (interaction with the target application to grasp its inherent context and input-output relationships), Payload Generation (devising a prompt generation plan based on the obtained application context and prompt injection guidelines), and Feedback (gauging the effectiveness of the attack by scrutinizing the LLMâ€™s responses to the injected prompts, followed by iterative refinement for optimal outcomes), aiming to trick LLMs into interpreting malicious payloads as questions rather than data payloads. Experiments on 36 real-world LLMintegrated services using HOUYI show an 86.1% success rate in launching attacks, revealing severe ramifications such as unauthorized imitation of services and exploitation of computational power.