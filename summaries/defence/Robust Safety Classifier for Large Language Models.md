On the classifier-based side, Kim et al. (2023) propose the Adversarial Prompt Shield (APS), a DistilBERT(Sanh et al., 2019)-based model designed for prompt classification into safe or unsafe categories. This approach is complemented by a method for generating training data that simulates adversarial attacks by adding synthetic noise to legitimate conversations. However, the necessity for frequent retraining to stay abreast of new attack vectors and reduce false positives presents a challenge to this approach.