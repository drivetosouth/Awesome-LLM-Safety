Chao et al. (2023) represent a strategic approach in jailbreaking, utilizing a minimal number of queries. This method doesn’t just exploit simple model vulnerabilities but involves a nuanced understanding of the model’s response mechanism, iteratively refining queries to probe and eventually bypass the model’s defenses. The success of this approach underscores a key vulnerability in LLMs: their predictability and manipulability through iterative, intelligent querying. This work introduces Prompt Automatic Iterative Refinement (PAIR), an algorithm designed to automate the generation of semantic jailbreaks for LLMs. PAIR works by using an attacker LLM to iteratively query a target LLM, refining a candidate jailbreak. This approach, more efficient than previous methods, requires fewer queries and can often produce a jailbreak in under twenty queries. PAIR demonstrates success in jailbreaking various LLMs, including GPT-3.5/4 and Vicuna, and is notable for its efficiency and interpretability, making the jailbreaks transferable to other LLMs. 