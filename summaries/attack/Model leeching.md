there is a specific type of model extraction a.k.a. “Model Leeching” (Birch et al., 2023b), where an attacker queries a “victim model” to extract knowledge from it. Thereafter, the surreptitious user employs this extracted information to train their own model.
The primary objective of model leeching is to gain insights from the victim model without directly accessing its internal parameters or architecture. Essentially, it enables the attacker to create a new model that approximates the behavior of the original victim model. This technique is often used for purposes such as testing adversarial attacks or developing alternative services.
It is worth noting that this approach allows for unrestricted testing of adversarial attacks, but its effectiveness heavily relies on the quality of the prompts. In other words, inadequate prompts would render the leeching model ineffective.
