Yong et al. (2023) exposes the heightened vulnerability of LLMs in processing low-resource languages. This indicates a significant gap in the models’ linguistic coverage and comprehension, especially for languages with limited representation in training data. This work demonstrated that by translating unsafe English inputs into low-resource languages, it’s possible to circumvent GPT-4’s safety safeguards.