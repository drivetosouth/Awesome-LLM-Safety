Chen et al. (2023) investigate whether fine-tuning large language models (LLMs) on small datasets containing personal identifiable information (PII) can lead to the models disclosing more PII embedded in their original training data. The authors demonstrate a strawman method where an LLM is fine-tuned on a small PII dataset converted to text, which enables the model to then disclose more PII when prompted. To improve on this, they propose Janus methodology which defines a PII recovery task and uses fewshot fine-tuning. Experiments indicate that finetuning GPT-3.5 on just 10 PII instances enables it to accurately disclose 650 out of 1000 target PIIs, versus 0 without fine-tuning. The Janus method further improves this divulgence, disclosing 699 target PIIs. Analysis shows larger models and real training data have stronger memorization and PII recovery and fine-tuning is more effective than prompt engineering alone for PII leakage. This indicates that LLMs can shift from non-disclosure to revealing significant amounts of PII with minimal fine-tuning.