Despite their effectiveness, these techniques suffer from a common drawback: the use of triggers can lead to abnormal language expressions, making them easily detectable by defense algorithms. To address this, ProAttack (Zhao et al., 2023a), a clean-label backdoor attack method, takes a different approach. Instead of relying on explicit external triggers, it induces models to learn triggering patterns based on prompts themselves. Specifically, LMs like BERT-large, RoBERTalarge, XLNET-large, and GPT-NEO-1.3B are all vulnerable to this attack, with GPT-NEO-1.3B being the most susceptible model. Zhao et al. hypothesis that prompts could function as triggers of backdoor attacks which corroborating with the observation that different prompts induce the model to learn different feature representation.