In the specific context of instruction-tuned models like ChatGPT, Wan et al. (Wan et al., 2023) investigate their susceptibility to data poisoning. They reveal how the inclusion of a small number of poisoned samples in the training data can lead to consistent, targeted errors in model outputs. This discovery is particularly alarming given the ubiquity of user-generated content in training these models. Wan et al.’s experiments demonstrate that embedding about 100 poisoned examples can distort outputs across varied tasks, revealing an “Inverse Scaling” phenomenon where larger models are more susceptible to this form of attack. Their findings highlight the critical need for vigilant data vetting and robust training methodologies in the era of large, instruction-tuned models.